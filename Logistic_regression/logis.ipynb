{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "###################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Importing some classes to perform various Ensemble techniques\n",
    "'''\n",
    "Let me briefly describe what each of these classes does:\n",
    "1. BaggingClassifier: Implements ensemble Bagging algorithms, such as Random Forest.\n",
    "\n",
    "2. RandomForestClassifier: Implements the Random Forest algorithm, which is an ensemble \n",
    "of decision trees trained with bagging.\n",
    "\n",
    "3. GradientBoostingClassifier: Implements gradient boosting, a technique that builds an \n",
    "ensemble of weak learners (often decision trees) in a sequential manner, where each new \n",
    "learner corrects errors made by the previous one.\n",
    "\n",
    "4. AdaBoostClassifier: Implements AdaBoost (Adaptive Boosting), a boosting algorithm that \n",
    "combines multiple weak learners (typically shallow decision trees) to create a strong classifier.\n",
    "\n",
    "5. VotingClassifier: Implements voting ensembles, which combine the predictions from \n",
    "multiple classifiers either by majority voting (hard voting) or by averaging the predicted \n",
    "probabilities (soft voting)\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# To perform Hyperparamter Tunning and Cross Validation of the classfication models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "###################################################\n",
    "#Space for making a Streamlit update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The dataset we will use is iris dataset which wil present in the seaborn module of python.\n",
    "1. The Iris dataset is a famous dataset in the field of machine learning and statistics.\n",
    "2. It contains information about various species of iris flowers, with measurements of their sepal length, sepal width, petal length, and petal width.\n",
    "3. These measurements are used to classify the iris flowers into three species: Setosa, Versicolor, and Virginica.\n",
    "'''\n",
    "iris_df=sns.load_dataset('iris')\n",
    "iris_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of code prints the shape of the Iris dataset, indicating the number of rows (instances) and features (columns) it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of Iris dataset is :{0} rows and {1} features.\".format(iris_df.shape[0],iris_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a code to find if we have null values in any of the features.\n",
    "pd.DataFrame(iris_df.isna().sum(),columns=['No of null values'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing multivariate and univariate analysis on the Iris dataset involves examining the relationships between multiple variables (multivariate analysis) and analyzing individual variables separately (univariate analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "sns.countplot(data=iris_df,x='species')\n",
    "plt.title('Count plot for No. of samples of each species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=iris_df,hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pair plot we can observe relationship between different features with each other. We dont need to perform any data preprocessing on the dataset as it as a 'Healthy Dataset' ie. it have no missing values and the distribution is aprrox normal. A \"healthy dataset\" with no missing values and a normal distribution can make the modeling process smoother and more straightforward. However, keep in mind that even in such cases, it's still a good practice to perform exploratory data analysis (EDA) to gain insights into the data and understand its characteristics better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding categorical variables is essential for machine learning as it converts categorical data into numerical form, enabling algorithms to process it effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder() #we use LabelEncoder because we have nominal categories\n",
    "iris_df['species']=encoder.fit_transform(iris_df['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a train-test split is crucial in machine learning for evaluating model performance. This step divides the dataset into training and testing sets, allowing the model to learn patterns from the training data and then assess its performance on unseen data from the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(iris_df.iloc[:,0:4],iris_df.iloc[:,-1],test_size=0.2,random_state=42)\n",
    "print(\"New shape of training dataset is : \",X_train.shape)\n",
    "print(\"New shape of testing dataset is : \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()\n",
    "dt=DecisionTreeClassifier()\n",
    "rf=RandomForestClassifier(n_estimators=100)\n",
    "gbc=GradientBoostingClassifier(n_estimators=100)\n",
    "ada=AdaBoostClassifier(estimator=Decision_tree,n_estimators=100)\n",
    "bag=BaggingClassifier(estimator=None,n_estimators=100)\n",
    "estimators=[lr,dt,rf,gbc,ada,bag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train,y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
